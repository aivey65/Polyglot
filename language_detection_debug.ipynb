{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus from https://www.kaggle.com/datasets/chazzer/big-language-detection-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess corpus data in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "topLang = [\"eng\", \"cmn\", \"hin\", \"spa\", \"fra\", \"ara\", \"ben\", \"rus\", \"por\", \"ind\", \"urd\", \"deu\", \"jpn\", \"swh\", \"pnb\", \"tam\", \"kor\", \"vie\", \"jav\", \"ita\", \"tha\", \"tgl\", \"pol\", \"yor\", \"ukr\", \"ibo\", \"npi\", \"ron\", \"nld\", \"zsm\", \"afr\", \"grc\", \"swe\", \"heb\", \"lat\", \"san\", \"gle\", \"mri\", \"chr\", \"nav\", \"haw\", \"smo\"]\n",
    "\n",
    "data = pd.read_csv(\"sentences.csv\")\n",
    "\n",
    "data.drop([\"id\"], axis=1, inplace=True)\n",
    "data = data[data[\"lan_code\"].isin(topLang)]\n",
    "\n",
    "data[\"sentence\"] = data.apply(lambda x: re.sub(r\"(?=[\\p{Common}])[^']|(?<![a-zA-Z])'|'(?![a-zA-Z])\", \" \", x[\"sentence\"].lower()), axis=1)\n",
    "\n",
    "\"\"\" To enforce all languages have an equal number of samples:\"\"\"\n",
    "# data = filtered_data.groupby(\"lan_code\").sample(28)\n",
    "\n",
    "\"\"\" To just ensure that language sample sizes are not larger than 1000:\"\"\"\n",
    "data = data[data.groupby(\"lan_code\").cumcount() < 1000]\n",
    "\n",
    "data.to_csv(\"language_corpus.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "accuracy: 0.963096168881939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "def returnSelf(x):\n",
    "    return x\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordPieceTrainer(vocab_size=50000, special_tokens=[\"[UNK]\"])\n",
    "tokenizer.train([\"language_corpus.csv\"], trainer)\n",
    "\n",
    "data = pd.read_csv(\"language_corpus.csv\")\n",
    "tokenized_data = tokenizer.encode_batch(data[\"sentence\"])\n",
    "\n",
    "tokenized_data = [i.tokens for i in tokenized_data]\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(data[\"lan_code\"])\n",
    "labels = le.transform(data[\"lan_code\"])\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(tokenized_data, labels, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), analyzer=returnSelf)\n",
    "vectorizer.fit(train_text)\n",
    "train_text = vectorizer.transform(train_text)\n",
    "test_text = vectorizer.transform(test_text)\n",
    "\n",
    "languageDetectionModel = ComplementNB()\n",
    "languageDetectionModel.fit(train_text, train_labels)\n",
    "\n",
    "predictions = languageDetectionModel.predict(test_text)\n",
    "\n",
    "ac = accuracy_score(test_labels, predictions)\n",
    "\n",
    "print(\"accuracy: \" + str(ac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
